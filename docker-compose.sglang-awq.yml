services:
  sglang-awq:
    image: lmsysorg/sglang:latest
    container_name: sglang-qwen-awq
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    ports:
      - "127.0.0.1:8000:8000"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
      - ./models:/models
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - HF_TOKEN=${HF_TOKEN:-}
      # RTX 5090 workaround
      - TORCH_CUDA_ARCH_LIST=7.0;7.5;8.0;8.6;8.9;9.0+PTX
      - CUDA_LAUNCH_BLOCKING=0
    command: >
      python -m sglang.launch_server
      --model-path Qwen/Qwen3-32B-AWQ
      --host 0.0.0.0
      --port 8000
      --dtype half
      --quantization awq_marlin
      --max-total-tokens 16384
      --mem-fraction-static 0.95
      --trust-remote-code
      --disable-cuda-graph
      --disable-custom-all-reduce
    restart: unless-stopped
    shm_size: '32gb'

networks:
  default:
    name: sglang-net
