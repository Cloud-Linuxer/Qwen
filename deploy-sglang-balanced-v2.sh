#!/bin/bash

# SGLang ê· í˜• ìµœì í™” v2
# ì•ˆì •ì„±ê³¼ ì„±ëŠ¥ì˜ ê· í˜•

echo "âš–ï¸ SGLang ê· í˜• ìµœì í™” v2 ë°°í¬..."
echo "ğŸ“‹ ê· í˜• ìµœì í™”:"
echo "  âœ… Torch Compile (ì•ˆì •ì  ì„¤ì •)"
echo "  âœ… ìŠ¤ë§ˆíŠ¸ ìŠ¤ì¼€ì¤„ë§ (LOF)"
echo "  âœ… ì¤‘ê°„ ì—°ì† ë””ì½”ë“œ"
echo "  âœ… ìµœì  ë©”ëª¨ë¦¬ ì„¤ì •"
echo "  âœ… ê°œì„ ëœ Triton ì„¤ì •"

# ê¸°ì¡´ ì»¨í…Œì´ë„ˆ ì •ë¦¬
docker stop sglang-balanced-v2 2>/dev/null
docker rm sglang-balanced-v2 2>/dev/null

# ê· í˜• ìµœì í™” ë°°í¬
docker run -d \
  --name sglang-balanced-v2 \
  --runtime nvidia \
  --gpus all \
  -p 8003:8000 \
  -v ~/.cache/huggingface:/root/.cache/huggingface \
  -e PYTORCH_CUDA_ALLOC_CONF="expandable_segments:True,max_split_size_mb:256" \
  -e CUDA_LAUNCH_BLOCKING=0 \
  -e OMP_NUM_THREADS=10 \
  -e TORCH_CUDA_ARCH_LIST="9.0;12.0+PTX" \
  -e CUDA_VISIBLE_DEVICES=0 \
  -e NCCL_P2P_DISABLE=0 \
  --shm-size 40g \
  sglang:blackwell-final-v2 \
  --model-path Qwen/Qwen3-32B-AWQ \
  --host 0.0.0.0 \
  --port 8000 \
  --quantization awq \
  --max-total-tokens 3328 \
  --max-prefill-tokens 1664 \
  --chunked-prefill-size 1152 \
  --mem-fraction-static 0.87 \
  --trust-remote-code \
  --attention-backend triton \
  --sampling-backend pytorch \
  --schedule-policy lof \
  --schedule-conservativeness 0.95 \
  --enable-torch-compile \
  --torch-compile-max-bs 6 \
  --num-continuous-decode-steps 2 \
  --triton-attention-reduce-in-fp32 \
  --triton-attention-num-kv-splits 12 \
  --disable-cuda-graph \
  --disable-custom-all-reduce \
  --disable-flashinfer \
  --disable-overlap-schedule \
  --decode-log-interval 30 \
  --stream-output \
  --stream-interval 1 \
  --allow-auto-truncate

echo "âœ… ê· í˜• ìµœì í™” v2 ì»¨í…Œì´ë„ˆ ì‹œì‘ë¨"
echo ""
echo "âš–ï¸ ê· í˜• ì„¤ì •:"
echo "  - Torch Compile: ë°°ì¹˜ í¬ê¸° 6"
echo "  - Schedule: LOF (Least Outstanding First)"
echo "  - ì—°ì† ë””ì½”ë“œ: 2 ìŠ¤í…"
echo "  - Triton KV ë¶„í• : 12"
echo "  - ìë™ íŠ¸ë ì¼€ì´íŠ¸: í™œì„±í™”"
echo "  - ë©”ëª¨ë¦¬: 87% ì •ì  í• ë‹¹"
echo ""
echo "ğŸ”— API ì—”ë“œí¬ì¸íŠ¸: http://localhost:8003"