#!/bin/bash

# SGLang ì„±ëŠ¥ ê°œì„  v2: Triton + Torch Compile + CUDA Graphs (small batch)

echo "ğŸš€ SGLang ì„±ëŠ¥ ê°œì„  v2 ë°°í¬..."
echo "ğŸ“‹ ìµœì í™”: Triton + Torch Compile + ì‘ì€ ë°°ì¹˜ CUDA Graphs"

# ê¸°ì¡´ ì»¨í…Œì´ë„ˆ ì •ë¦¬
docker stop sglang-perf-v2 2>/dev/null
docker rm sglang-perf-v2 2>/dev/null

# ìµœì í™”ëœ ì„¤ì •ìœ¼ë¡œ ë°°í¬
docker run -d \
  --name sglang-perf-v2 \
  --runtime nvidia \
  --gpus all \
  -p 8003:8000 \
  -v ~/.cache/huggingface:/root/.cache/huggingface \
  -e PYTORCH_CUDA_ALLOC_CONF="expandable_segments:True,max_split_size_mb:512" \
  -e CUDA_LAUNCH_BLOCKING=0 \
  -e OMP_NUM_THREADS=8 \
  -e CUDA_DEVICE_ORDER="PCI_BUS_ID" \
  -e TORCH_CUDA_ARCH_LIST="8.0;8.6;8.9;9.0;12.0+PTX" \
  --shm-size 32g \
  sglang:blackwell-final-v2 \
  --model-path Qwen/Qwen3-32B-AWQ \
  --host 0.0.0.0 \
  --port 8000 \
  --quantization awq \
  --max-total-tokens 3072 \
  --max-prefill-tokens 1536 \
  --chunked-prefill-size 1024 \
  --mem-fraction-static 0.85 \
  --trust-remote-code \
  --attention-backend triton \
  --sampling-backend pytorch \
  --cuda-graph-max-bs 2 \
  --cuda-graph-bs 1 2 \
  --disable-custom-all-reduce \
  --disable-flashinfer \
  --disable-radix-cache \
  --enable-torch-compile \
  --torch-compile-max-bs 4 \
  --num-continuous-decode-steps 2 \
  --decode-log-interval 40

echo "âœ… ì»¨í…Œì´ë„ˆ ì‹œì‘. ë¡œê·¸ í™•ì¸..."
sleep 5
docker logs sglang-perf-v2 --tail 20