#!/bin/bash
# RTX 5090 ìµœì¢… ë°°í¬ ìŠ¤í¬ë¦½íŠ¸ - CUDA 12.8 + PyTorch 2.7.0
# Qwen3-32B-AWQ ëª¨ë¸ ì„œë¹™

set -e

echo "=== RTX 5090 Blackwell ìµœì¢… ë°°í¬ ==="
echo "CUDA 12.8 + PyTorch 2.7.0 ì •ì‹ ì§€ì›"
echo "====================================="
echo

# ìƒ‰ìƒ ì½”ë“œ
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

# GPU í™•ì¸
echo -e "${BLUE}ğŸ“Š GPU ì •ë³´:${NC}"
nvidia-smi --query-gpu=name,memory.total,memory.free,driver_version,compute_cap --format=csv,noheader
echo

# ê¸°ì¡´ ì»¨í…Œì´ë„ˆ ì •ë¦¬
echo "ğŸ§¹ ê¸°ì¡´ ì»¨í…Œì´ë„ˆ ì •ë¦¬..."
docker ps -aq --filter "name=rtx5090\|sglang\|qwen" | xargs -r docker stop 2>/dev/null || true
docker ps -aq --filter "name=rtx5090\|sglang\|qwen" | xargs -r docker rm 2>/dev/null || true
echo

# Docker ì´ë¯¸ì§€ ë¹Œë“œ
echo -e "${BLUE}ğŸ”¨ RTX 5090 ì •ì‹ ì§€ì› ì´ë¯¸ì§€ ë¹Œë“œ ì¤‘...${NC}"
echo "CUDA 12.8 + PyTorch 2.7.0 ì‚¬ìš©"
docker build -f Dockerfile.rtx5090-cuda128 -t rtx5090-sglang:latest . || {
    echo -e "${RED}âŒ ì´ë¯¸ì§€ ë¹Œë“œ ì‹¤íŒ¨${NC}"
    exit 1
}
echo -e "${GREEN}âœ… ì´ë¯¸ì§€ ë¹Œë“œ ì™„ë£Œ!${NC}"
echo

# Docker Compose íŒŒì¼ ìƒì„±
echo "ğŸ“ Docker Compose ì„¤ì • ìƒì„±..."
cat > docker-compose.rtx5090-final.yml << 'EOF'
services:
  rtx5090-sglang:
    image: rtx5090-sglang:latest
    container_name: rtx5090-qwen-awq
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    ports:
      - "127.0.0.1:8000:8000"
      - "127.0.0.1:8001:8001"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
      - ./models:/models
      - /tmp:/tmp
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - HF_TOKEN=${HF_TOKEN:-}
      - TORCH_CUDA_ARCH_LIST=7.0;7.5;8.0;8.6;8.9;9.0;12.0+PTX
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    command:
      - --model-path=Qwen/Qwen3-32B-AWQ
      - --host=0.0.0.0
      - --port=8000
      - --dtype=half
      - --quantization=awq_marlin
      - --max-total-tokens=16384
      - --mem-fraction-static=0.95
      - --trust-remote-code
      - --chunked-prefill-size=1024
      - --enable-torch-compile
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 300s
    restart: unless-stopped
    shm_size: '32gb'
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536

networks:
  default:
    name: rtx5090-network
    driver: bridge
EOF
echo -e "${GREEN}âœ… ì„¤ì • ì™„ë£Œ${NC}"
echo

# .env íŒŒì¼ ìƒì„±
if [ ! -f .env ]; then
    cat > .env << 'EOF'
# Hugging Face Token (ì„ íƒì‚¬í•­)
HF_TOKEN=

# ì˜¤í”„ë¼ì¸ ëª¨ë“œ (1ë¡œ ì„¤ì •ì‹œ ìºì‹œëœ ëª¨ë¸ë§Œ ì‚¬ìš©)
HF_HUB_OFFLINE=0
EOF
    echo "ğŸ“ .env íŒŒì¼ ìƒì„±ë¨"
fi

# ì„œë¹„ìŠ¤ ì‹œì‘
echo -e "${BLUE}ğŸš€ SGLang ì„œë²„ ì‹œì‘...${NC}"
docker compose -f docker-compose.rtx5090-final.yml up -d

# ìƒíƒœ í™•ì¸
echo "â³ ì„œë²„ ì´ˆê¸°í™” ëŒ€ê¸°ì¤‘..."
sleep 10

# ë¡œê·¸ í™•ì¸ (ë°±ê·¸ë¼ìš´ë“œ)
(
    MAX_RETRIES=60
    RETRY_COUNT=0
    while [ $RETRY_COUNT -lt $MAX_RETRIES ]; do
        if curl -s http://localhost:8000/health > /dev/null 2>&1; then
            echo
            echo -e "${GREEN}âœ… ì„œë²„ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!${NC}"
            break
        fi
        sleep 5
        RETRY_COUNT=$((RETRY_COUNT + 1))
    done
) &

# ì‹¤ì‹œê°„ ë¡œê·¸ í‘œì‹œ
echo
echo "ğŸ“‹ ì„œë²„ ë¡œê·¸:"
echo "----------------------------------------"
timeout 30 docker compose -f docker-compose.rtx5090-final.yml logs -f 2>&1 | while IFS= read -r line; do
    if echo "$line" | grep -q "Model loaded\|Server started\|ready\|8000"; then
        echo -e "${GREEN}$line${NC}"
        break
    elif echo "$line" | grep -q "error\|ERROR\|failed"; then
        echo -e "${RED}$line${NC}"
    elif echo "$line" | grep -q "warning\|WARNING"; then
        echo -e "${YELLOW}$line${NC}"
    else
        echo "$line"
    fi
done || true

echo
echo "=========================================="
echo

# ì„œë¹„ìŠ¤ ìƒíƒœ
docker compose -f docker-compose.rtx5090-final.yml ps

echo
echo -e "${BLUE}=== API ì—”ë“œí¬ì¸íŠ¸ ===${NC}"
echo "ğŸ“ ë©”ì¸ API: http://localhost:8000"
echo "ğŸ“ í—¬ìŠ¤ì²´í¬: http://localhost:8000/health"
echo "ğŸ“ ëª¨ë¸ ì •ë³´: http://localhost:8000/v1/models"
echo

# API í…ŒìŠ¤íŠ¸
echo -e "${BLUE}ğŸ§ª API í…ŒìŠ¤íŠ¸...${NC}"
sleep 5
curl -s -X POST http://localhost:8000/v1/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "Qwen/Qwen3-32B-AWQ",
        "prompt": "RTX 5090ì˜ ì¥ì ì€",
        "max_tokens": 50,
        "temperature": 0.7
    }' | jq -r '.choices[0].text' 2>/dev/null || echo "API ì¤€ë¹„ ì¤‘..."

echo
echo -e "${BLUE}=== ëª…ë ¹ì–´ ===${NC}"
echo "ë¡œê·¸: docker compose -f docker-compose.rtx5090-final.yml logs -f"
echo "ì¤‘ì§€: docker compose -f docker-compose.rtx5090-final.yml down"
echo "GPU: watch -n 1 nvidia-smi"
echo

echo -e "${GREEN}ğŸ‰ RTX 5090 ë°°í¬ ì™„ë£Œ!${NC}"
echo "CUDA 12.8 + PyTorch 2.7.0 ì •ì‹ ì§€ì›"