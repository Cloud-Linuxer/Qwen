version: '3.8'

services:
  qwen3-32b-awq:
    image: lmsysorg/sglang:latest
    container_name: qwen3-32b-awq-sglang
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    ports:
      - "127.0.0.1:8000:8000"
      - "127.0.0.1:8001:8001"  # 메트릭스
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
      - ./models:/models
      - /tmp:/tmp
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - HF_TOKEN=${HF_TOKEN:-}
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN:-}
      - HF_HUB_OFFLINE=${HF_HUB_OFFLINE:-0}
      # RTX 5090 최적화
      - TORCH_CUDA_ARCH_LIST=7.0;7.5;8.0;8.6;8.9;9.0;12.0+PTX
      - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
      # AWQ 양자화 설정
      - VLLM_USE_MODELSCOPE=0
      - QUANTIZATION_METHOD=awq
    command: >
      python -m sglang.launch_server
      --model-path Qwen/Qwen3-32B-AWQ
      --host 0.0.0.0
      --port 8000
      --dtype half
      --quantization awq
      --max-total-tokens 16384
      --mem-fraction-static 0.90
      --trust-remote-code
      --enable-torch-compile
      --cuda-graph-max-bs 32
      --chunked-prefill-size 512
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 300s
    restart: unless-stopped
    shm_size: '32gb'
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536

networks:
  default:
    name: qwen3-awq-network
    driver: bridge
