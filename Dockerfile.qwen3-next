# Qwen3-Next-80B-A3B-Instruct Deployment
FROM nvidia/cuda:12.6.3-runtime-ubuntu22.04

# Install Python and system dependencies
RUN apt-get update && apt-get install -y \
    python3.10 \
    python3-pip \
    git \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Set Python 3.10 as default
RUN update-alternatives --install /usr/bin/python python /usr/bin/python3.10 1
RUN update-alternatives --install /usr/bin/pip pip /usr/bin/pip3 1

# Upgrade pip
RUN pip install --upgrade pip

# Install latest transformers from source and vLLM
RUN pip install git+https://github.com/huggingface/transformers.git \
    vllm==0.6.6.post1 \
    accelerate \
    auto-gptq \
    optimum \
    bitsandbytes

# RTX 5090 compatibility workaround
ENV TORCH_CUDA_ARCH_LIST="8.0;8.6;8.9;9.0+PTX"
ENV PYTORCH_NO_CUDA_MEMORY_CACHING=1
ENV VLLM_WORKER_MULTIPROC_METHOD=spawn

# Model cache directory
ENV HF_HOME=/models
ENV TRANSFORMERS_CACHE=/models

# Create model directory
RUN mkdir -p /models

# Working directory
WORKDIR /app

# Expose API port
EXPOSE 8000

# Health check
HEALTHCHECK --interval=30s --timeout=10s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# Default command with quantization and offloading
CMD ["python", "-m", "vllm.entrypoints.openai.api_server", \
     "--model", "Qwen/Qwen3-Next-80B-A3B-Instruct", \
     "--dtype", "bfloat16", \
     "--quantization", "gptq", \
     "--gpu-memory-utilization", "0.95", \
     "--cpu-offload-gb", "100", \
     "--max-model-len", "32768", \
     "--tensor-parallel-size", "1", \
     "--host", "0.0.0.0", \
     "--port", "8000", \
     "--disable-custom-all-reduce"]