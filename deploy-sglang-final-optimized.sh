#!/bin/bash

# SGLang ìµœì¢… ìµœì í™” ë²„ì „
# ì•ˆì •ì ìœ¼ë¡œ ì‘ë™í•˜ëŠ” ìµœì í™”ë§Œ í¬í•¨

echo "ğŸš€ SGLang ìµœì¢… ìµœì í™” ë²„ì „ ë°°í¬..."
echo "ğŸ“‹ ì ìš© ìµœì í™”:"
echo "  âœ… Triton attention backend"
echo "  âœ… CUDA ë¹„ë™ê¸° ì‹¤í–‰ (CUDA_LAUNCH_BLOCKING=0)"
echo "  âœ… ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì •"
echo "  âœ… ì¦ê°€ëœ í† í° í•œê³„ (3072)"
echo "  âœ… PyTorch ìƒ˜í”Œë§ ë°±ì—”ë“œ"

# ê¸°ì¡´ ì»¨í…Œì´ë„ˆ ì •ë¦¬
docker stop sglang-final 2>/dev/null
docker rm sglang-final 2>/dev/null

# ìµœì¢… ìµœì í™” ë°°í¬
docker run -d \
  --name sglang-final \
  --runtime nvidia \
  --gpus all \
  -p 8000:8000 \
  -v ~/.cache/huggingface:/root/.cache/huggingface \
  -e PYTORCH_CUDA_ALLOC_CONF="expandable_segments:True,max_split_size_mb:256" \
  -e CUDA_LAUNCH_BLOCKING=0 \
  -e OMP_NUM_THREADS=8 \
  --shm-size 32g \
  sglang:blackwell-final-v2 \
  --model-path Qwen/Qwen3-32B-AWQ \
  --host 0.0.0.0 \
  --port 8000 \
  --quantization awq \
  --max-total-tokens 3072 \
  --max-prefill-tokens 1536 \
  --chunked-prefill-size 1024 \
  --mem-fraction-static 0.85 \
  --trust-remote-code \
  --attention-backend triton \
  --sampling-backend pytorch \
  --disable-cuda-graph \
  --disable-custom-all-reduce \
  --disable-flashinfer \
  --disable-radix-cache \
  --decode-log-interval 40 \
  --stream-output

echo "âœ… ì»¨í…Œì´ë„ˆ ì‹œì‘ë¨"
echo ""
echo "ğŸ“Š ì˜ˆìƒ ì„±ëŠ¥:"
echo "  - ì²˜ë¦¬ ì†ë„: ~10 tokens/s"
echo "  - ì§§ì€ ì‘ë‹µ: ~1ì´ˆ"
echo "  - ì¤‘ê°„ ì‘ë‹µ (50 í† í°): ~5ì´ˆ"
echo "  - ê¸´ ì‘ë‹µ (100 í† í°): ~10ì´ˆ"
echo ""
echo "ğŸ”— API ì—”ë“œí¬ì¸íŠ¸: http://localhost:8000"
echo ""
echo "í…ŒìŠ¤íŠ¸ ëª…ë ¹:"
echo "  curl -X POST http://localhost:8000/v1/completions \\"
echo "    -H 'Content-Type: application/json' \\"
echo "    -d '{\"model\": \"Qwen/Qwen3-32B-AWQ\", \"prompt\": \"Hello\", \"max_tokens\": 10}'"